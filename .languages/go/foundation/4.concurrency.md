# Go Concurrency

## Core Philosophy
**"Don't communicate by sharing memory; share memory by communicating."** - Go's concurrency model is built around goroutines and channels, providing CSP (Communicating Sequential Processes) style concurrency.

## Goroutines

### What are Goroutines?
Goroutines are lightweight threads managed by the Go runtime. They are:
- **Cheap**: ~2KB initial stack size (grows/shrinks as needed)
- **Fast**: Faster creation than OS threads
- **Multiplexed**: Many goroutines run on fewer OS threads

### Creating Goroutines
```go
// Start a goroutine
go func() {
    fmt.Println("Running in goroutine")
}()

// Start goroutine with function
go myFunction()

// Start goroutine with parameters
go func(name string, id int) {
    fmt.Printf("Worker %s with id %d\n", name, id)
}("worker", 1)
```

### Goroutine Lifecycle
```go
import (
    "context"
    "sync"
    "time"
)

func goroutineLifecycle() {
    var wg sync.WaitGroup
    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
    defer cancel()
    
    // Start multiple goroutines
    for i := 0; i < 5; i++ {
        wg.Add(1)
        go worker(ctx, i, &wg)
    }
    
    // Wait for all goroutines to complete
    wg.Wait()
}

func worker(ctx context.Context, id int, wg *sync.WaitGroup) {
    defer wg.Done()
    
    select {
    case <-time.After(time.Duration(id) * time.Second):
        fmt.Printf("Worker %d completed\n", id)
    case <-ctx.Done():
        fmt.Printf("Worker %d cancelled\n", id)
    }
}
```

## Channels

### Channel Basics
```go
// Unbuffered channel
ch := make(chan int)

// Buffered channel
ch := make(chan int, 5)

// Send operation
ch <- 42

// Receive operation
value := <-ch

// Receive with ok flag
value, ok := <-ch
if !ok {
    fmt.Println("Channel closed")
}

// Close channel
close(ch)
```

### Channel Types and Directions

#### Bidirectional Channels
```go
ch := make(chan string)  // Can send and receive
```

#### Send-only Channels
```go
func sender(ch chan<- string) {
    ch <- "message"
    // <-ch would cause compile error
}
```

#### Receive-only Channels
```go
func receiver(ch <-chan string) {
    msg := <-ch
    // ch <- "message" would cause compile error
}
```

### Buffered vs Unbuffered Channels

#### Unbuffered (Synchronous)
```go
ch := make(chan int)

go func() {
    ch <- 42  // Blocks until someone receives
}()

value := <-ch  // Blocks until someone sends
```

#### Buffered (Asynchronous)
```go
ch := make(chan int, 3)

ch <- 1  // Doesn't block (buffer has space)
ch <- 2  // Doesn't block
ch <- 3  // Doesn't block
// ch <- 4 would block (buffer full)

value := <-ch  // Gets 1, doesn't block
```

### Channel Patterns

#### Fan-In Pattern
```go
func fanIn(input1, input2 <-chan string) <-chan string {
    output := make(chan string)
    
    go func() {
        defer close(output)
        for {
            select {
            case msg, ok := <-input1:
                if !ok {
                    input1 = nil
                    continue
                }
                output <- msg
            case msg, ok := <-input2:
                if !ok {
                    input2 = nil
                    continue
                }
                output <- msg
            }
            
            if input1 == nil && input2 == nil {
                break
            }
        }
    }()
    
    return output
}
```

#### Fan-Out Pattern
```go
func fanOut(input <-chan int, workers int) []<-chan int {
    outputs := make([]<-chan int, workers)
    
    for i := 0; i < workers; i++ {
        output := make(chan int)
        outputs[i] = output
        
        go func(out chan<- int) {
            defer close(out)
            for value := range input {
                out <- process(value)
            }
        }(output)
    }
    
    return outputs
}
```

#### Pipeline Pattern
```go
func pipeline() {
    // Stage 1: Generate numbers
    numbers := make(chan int)
    go func() {
        defer close(numbers)
        for i := 1; i <= 100; i++ {
            numbers <- i
        }
    }()
    
    // Stage 2: Square numbers
    squares := make(chan int)
    go func() {
        defer close(squares)
        for num := range numbers {
            squares <- num * num
        }
    }()
    
    // Stage 3: Print results
    for square := range squares {
        fmt.Println(square)
    }
}
```

#### Worker Pool Pattern
```go
type Job struct {
    ID   int
    Data interface{}
}

type Result struct {
    JobID int
    Value interface{}
    Error error
}

func workerPool(jobs <-chan Job, results chan<- Result, numWorkers int) {
    var wg sync.WaitGroup
    
    // Start workers
    for i := 0; i < numWorkers; i++ {
        wg.Add(1)
        go worker(i, jobs, results, &wg)
    }
    
    // Close results channel when all workers done
    go func() {
        wg.Wait()
        close(results)
    }()
}

func worker(id int, jobs <-chan Job, results chan<- Result, wg *sync.WaitGroup) {
    defer wg.Done()
    
    for job := range jobs {
        // Process job
        value, err := processJob(job)
        
        results <- Result{
            JobID: job.ID,
            Value: value,
            Error: err,
        }
    }
}
```

## Select Statement

### Basic Select
```go
select {
case msg1 := <-ch1:
    fmt.Println("Received from ch1:", msg1)
case msg2 := <-ch2:
    fmt.Println("Received from ch2:", msg2)
case <-time.After(1 * time.Second):
    fmt.Println("Timeout!")
default:
    fmt.Println("No channels ready")
}
```

### Select Patterns

#### Timeout Pattern
```go
func withTimeout(ch <-chan string, timeout time.Duration) (string, error) {
    select {
    case result := <-ch:
        return result, nil
    case <-time.After(timeout):
        return "", errors.New("operation timed out")
    }
}
```

#### Context Cancellation
```go
func cancellableWork(ctx context.Context, work <-chan Work) {
    for {
        select {
        case <-ctx.Done():
            fmt.Println("Work cancelled")
            return
        case w := <-work:
            processWork(w)
        }
    }
}
```

#### Quit Channel Pattern
```go
func workWithQuit(work <-chan int, quit <-chan bool) {
    for {
        select {
        case job := <-work:
            fmt.Printf("Processing job: %d\n", job)
        case <-quit:
            fmt.Println("Stopping worker")
            return
        }
    }
}
```

## Context Package

### Context Types
```go
import "context"

// Background context (never cancelled)
ctx := context.Background()

// TODO context (placeholder)
ctx := context.TODO()

// With cancellation
ctx, cancel := context.WithCancel(parent)
defer cancel()

// With timeout
ctx, cancel := context.WithTimeout(parent, 30*time.Second)
defer cancel()

// With deadline
deadline := time.Now().Add(1 * time.Minute)
ctx, cancel := context.WithDeadline(parent, deadline)
defer cancel()

// With value
ctx := context.WithValue(parent, "key", "value")
```

### Context Best Practices
```go
func contextBestPractices(ctx context.Context, data []byte) error {
    // Always check for cancellation
    select {
    case <-ctx.Done():
        return ctx.Err()
    default:
    }
    
    // Pass context to long-running operations
    result, err := database.QueryContext(ctx, "SELECT * FROM users")
    if err != nil {
        return err
    }
    
    // Check cancellation in loops
    for _, item := range result {
        select {
        case <-ctx.Done():
            return ctx.Err()
        default:
        }
        
        if err := processItem(ctx, item); err != nil {
            return err
        }
    }
    
    return nil
}
```

## Synchronization Primitives

### Mutex
```go
import "sync"

type Counter struct {
    mu    sync.Mutex
    count int
}

func (c *Counter) Increment() {
    c.mu.Lock()
    defer c.mu.Unlock()
    c.count++
}

func (c *Counter) Value() int {
    c.mu.Lock()
    defer c.mu.Unlock()
    return c.count
}
```

### RWMutex
```go
type Cache struct {
    mu   sync.RWMutex
    data map[string]interface{}
}

func (c *Cache) Get(key string) (interface{}, bool) {
    c.mu.RLock()
    defer c.mu.RUnlock()
    value, exists := c.data[key]
    return value, exists
}

func (c *Cache) Set(key string, value interface{}) {
    c.mu.Lock()
    defer c.mu.Unlock()
    c.data[key] = value
}
```

### WaitGroup
```go
func waitGroupExample() {
    var wg sync.WaitGroup
    
    for i := 0; i < 5; i++ {
        wg.Add(1)
        go func(id int) {
            defer wg.Done()
            fmt.Printf("Worker %d starting\n", id)
            time.Sleep(time.Second)
            fmt.Printf("Worker %d done\n", id)
        }(i)
    }
    
    wg.Wait()
    fmt.Println("All workers completed")
}
```

### Once
```go
import "sync"

var (
    config Config
    once   sync.Once
)

func GetConfig() Config {
    once.Do(func() {
        config = loadConfig()  // Only called once
    })
    return config
}
```

### Atomic Operations
```go
import "sync/atomic"

type AtomicCounter struct {
    count int64
}

func (c *AtomicCounter) Increment() {
    atomic.AddInt64(&c.count, 1)
}

func (c *AtomicCounter) Value() int64 {
    return atomic.LoadInt64(&c.count)
}

func (c *AtomicCounter) CompareAndSwap(old, new int64) bool {
    return atomic.CompareAndSwapInt64(&c.count, old, new)
}
```

## Advanced Concurrency Patterns

### Semaphore Pattern
```go
type Semaphore struct {
    semaCh chan struct{}
}

func NewSemaphore(maxConcurrency int) *Semaphore {
    return &Semaphore{
        semaCh: make(chan struct{}, maxConcurrency),
    }
}

func (s *Semaphore) Acquire() {
    s.semaCh <- struct{}{}
}

func (s *Semaphore) Release() {
    <-s.semaCh
}

func (s *Semaphore) TryAcquire() bool {
    select {
    case s.semaCh <- struct{}{}:
        return true
    default:
        return false
    }
}
```

### Rate Limiting
```go
import "golang.org/x/time/rate"

type RateLimiter struct {
    limiter *rate.Limiter
}

func NewRateLimiter(rps int, burst int) *RateLimiter {
    return &RateLimiter{
        limiter: rate.NewLimiter(rate.Limit(rps), burst),
    }
}

func (rl *RateLimiter) Allow() bool {
    return rl.limiter.Allow()
}

func (rl *RateLimiter) Wait(ctx context.Context) error {
    return rl.limiter.Wait(ctx)
}
```

### Circuit Breaker Pattern
```go
type CircuitBreaker struct {
    mu           sync.Mutex
    failureCount int
    lastFailTime time.Time
    state        string // "closed", "open", "half-open"
    threshold    int
    timeout      time.Duration
}

func (cb *CircuitBreaker) Call(fn func() error) error {
    cb.mu.Lock()
    defer cb.mu.Unlock()
    
    if cb.state == "open" {
        if time.Since(cb.lastFailTime) > cb.timeout {
            cb.state = "half-open"
            cb.failureCount = 0
        } else {
            return errors.New("circuit breaker is open")
        }
    }
    
    err := fn()
    if err != nil {
        cb.failureCount++
        cb.lastFailTime = time.Now()
        
        if cb.failureCount >= cb.threshold {
            cb.state = "open"
        }
        return err
    }
    
    cb.failureCount = 0
    cb.state = "closed"
    return nil
}
```

### Graceful Shutdown
```go
func gracefulShutdown() {
    ctx, cancel := context.WithCancel(context.Background())
    
    // Start servers
    server := &http.Server{Addr: ":8080"}
    
    go func() {
        if err := server.ListenAndServe(); err != http.ErrServerClosed {
            log.Printf("Server error: %v", err)
        }
    }()
    
    // Start background workers
    var wg sync.WaitGroup
    for i := 0; i < 3; i++ {
        wg.Add(1)
        go backgroundWorker(ctx, &wg)
    }
    
    // Wait for interrupt signal
    sigChan := make(chan os.Signal, 1)
    signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)
    <-sigChan
    
    // Initiate shutdown
    log.Println("Shutting down...")
    
    // Cancel context to stop workers
    cancel()
    
    // Shutdown server with timeout
    shutdownCtx, shutdownCancel := context.WithTimeout(context.Background(), 30*time.Second)
    defer shutdownCancel()
    
    if err := server.Shutdown(shutdownCtx); err != nil {
        log.Printf("Server shutdown error: %v", err)
    }
    
    // Wait for workers to finish
    wg.Wait()
    log.Println("Shutdown complete")
}
```

## Testing Concurrent Code

### Race Detection
```bash
go test -race
go run -race main.go
```

### Testing Goroutines
```go
func TestConcurrentCounter(t *testing.T) {
    counter := &Counter{}
    var wg sync.WaitGroup
    
    // Start multiple goroutines
    for i := 0; i < 100; i++ {
        wg.Add(1)
        go func() {
            defer wg.Done()
            counter.Increment()
        }()
    }
    
    wg.Wait()
    
    if counter.Value() != 100 {
        t.Errorf("Expected 100, got %d", counter.Value())
    }
}
```

### Testing Channels
```go
func TestChannelTimeout(t *testing.T) {
    ch := make(chan string)
    
    select {
    case <-ch:
        t.Error("Should not receive from empty channel")
    case <-time.After(10 * time.Millisecond):
        // Expected timeout
    }
}
```

## Performance Considerations

### Goroutine Overhead
```go
// Avoid creating too many goroutines
func badApproach(tasks []Task) {
    for _, task := range tasks {
        go process(task)  // Could create millions of goroutines
    }
}

// Use worker pool instead
func goodApproach(tasks []Task) {
    jobs := make(chan Task, len(tasks))
    results := make(chan Result, len(tasks))
    
    // Start limited number of workers
    for i := 0; i < runtime.NumCPU(); i++ {
        go worker(jobs, results)
    }
    
    // Send jobs
    for _, task := range tasks {
        jobs <- task
    }
    close(jobs)
    
    // Collect results
    for range tasks {
        <-results
    }
}
```

### Channel Buffer Sizing
```go
// Buffer size affects performance
func channelBuffering() {
    // Too small: frequent blocking
    ch1 := make(chan int, 1)
    
    // Too large: memory waste
    ch2 := make(chan int, 10000)
    
    // Right size: based on expected throughput
    ch3 := make(chan int, runtime.NumCPU()*2)
}
```

## Common Pitfalls and Solutions

### 1. Goroutine Leaks
```go
// Bad: Goroutine may never exit
func badGoroutine() {
    ch := make(chan int)
    go func() {
        for i := range ch {  // Blocks forever if ch never closed
            process(i)
        }
    }()
}

// Good: Use context for cancellation
func goodGoroutine(ctx context.Context) {
    ch := make(chan int)
    go func() {
        defer close(ch)
        for {
            select {
            case <-ctx.Done():
                return
            case i := <-ch:
                process(i)
            }
        }
    }()
}
```

### 2. Shared Memory Without Synchronization
```go
// Bad: Data race
type BadCounter struct {
    count int
}

func (c *BadCounter) Increment() {
    c.count++  // Race condition!
}

// Good: Use channels or mutex
type GoodCounter struct {
    ch chan int
}

func (c *GoodCounter) Increment() {
    c.ch <- 1
}
```

### 3. Closing Channels Multiple Times
```go
// Bad: May panic
func badClose(ch chan int) {
    close(ch)
    close(ch)  // Panic!
}

// Good: Use sync.Once
func goodClose(ch chan int, once *sync.Once) {
    once.Do(func() {
        close(ch)
    })
}
```

## Best Practices

1. **Start with channels, use mutexes when channels don't fit**
2. **Always handle goroutine lifecycle (cancellation/timeout)**
3. **Use context for cancellation and deadlines**
4. **Prefer composition over inheritance for concurrent types**
5. **Test concurrent code with race detector**
6. **Monitor goroutine count in production**
7. **Use buffered channels for known capacity scenarios**
8. **Close channels from sender side**
9. **Use select default case for non-blocking operations**
10. **Avoid sharing memory; prefer message passing**

## References
- [Go Concurrency Patterns](https://go.dev/blog/pipelines)
- [Advanced Go Concurrency](https://go.dev/blog/advanced-go-concurrency-patterns)
- [Context Package](https://go.dev/blog/context)
- [Memory Model](https://go.dev/ref/mem)
- [Race Detector](https://go.dev/blog/race-detector)